{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from itertools import combinations, product\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        '''You can change this for your player if you need to handle state/have memory'''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self, game: 'Tic_Tac_Toe',id: int) -> tuple[int, int]:\n",
    "        '''\n",
    "        game: the Quixo game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions and a move among TOP, BOTTOM, LEFT and RIGHT\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class Tic_Tac_Toe(object):\n",
    "    def __init__(self) -> None:\n",
    "        self._board = np.array([[1, 6, 5], [8, 4, 0], [3, 2, 7]])\n",
    "        self._o_cells = [] # start the game  (player_id = 0)\n",
    "        self._x_cells = [] # play for second (player_id = 1)\n",
    "        \n",
    "\n",
    "    def get_state(self):\n",
    "        return (deepcopy(sorted(self._o_cells)), deepcopy(sorted(self._x_cells)))\n",
    "\n",
    "    def draw(self):\n",
    "        return ((len(self._x_cells) + len(self._o_cells)) == 9) and self.check_winner() == -1\n",
    "\n",
    "    def won(self, cells):\n",
    "        return any(sum(h) == 12 for h in combinations(cells, 3))\n",
    "\n",
    "    def check_winner(self) -> int:\n",
    "        if self.won(self._o_cells):\n",
    "            return 0\n",
    "        elif self.won(self._x_cells):\n",
    "            return 1\n",
    "        return -1\n",
    "\n",
    "    def print(self):\n",
    "        '''Prints the board. -1 are neutral pieces, 0 are pieces of player 0, 1 pieces of player 1'''\n",
    "\n",
    "        pretty_board = np.chararray(self._board.shape, itemsize=1, unicode=True)\n",
    "        for r in range(self._board.shape[0]):\n",
    "            for c in range(self._board.shape[1]):\n",
    "                if self._board[r, c] in self._x_cells:\n",
    "                    pretty_board[(r, c)] = '❌'\n",
    "                elif self._board[r, c] in self._o_cells:\n",
    "                    pretty_board[(r, c)] = '⭕'\n",
    "                else:\n",
    "                    pretty_board[(r, c)] = '⬜'\n",
    "\n",
    "        print(f'Board:\\n{pretty_board}')\n",
    "\n",
    "    def play(self, player1: Player, player2: Player,view=True) -> int:\n",
    "        '''Play the game. Returns the winning player'''\n",
    "        players = [player1, player2]\n",
    "        current_player_idx = 1\n",
    "        winner = -1\n",
    "        draw = False\n",
    "        while winner < 0 and not draw:\n",
    "            current_player_idx += 1\n",
    "            current_player_idx %= len(players)\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                pos = players[current_player_idx].make_move(self,current_player_idx)\n",
    "                ok = self.move(pos, current_player_idx)\n",
    "            winner = self.check_winner()\n",
    "            draw = self.draw()\n",
    "            if view:\n",
    "                self.print()\n",
    "        return winner\n",
    "\n",
    "    def move(self, pos: tuple[int, int], player_id: int) -> bool:\n",
    "        '''Perform a move'''\n",
    "        if player_id > 1:\n",
    "            return False\n",
    "        acceptable: bool = self.valid_move(pos)\n",
    "        if acceptable:\n",
    "            # put the player id in the piece\n",
    "            if player_id == 0:\n",
    "                self._o_cells.append(self._board[pos])\n",
    "            elif player_id == 1:\n",
    "                self._x_cells.append(self._board[pos])\n",
    "        return acceptable\n",
    "\n",
    "    def valid_move(self, pos):\n",
    "        return (self._board[pos] not in self._x_cells) and (self._board[pos] not in self._o_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.__name__ = 'Random Player'\n",
    "\n",
    "\n",
    "    def make_move(self, game: 'Tic_Tac_Toe',id:int) -> tuple[tuple[int, int]]:\n",
    "        # choose random position (row,col)\n",
    "        pos = (random.randint(0, 2), random.randint(0, 2))\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningPlayer(Player):\n",
    "    def __init__(self, num_episodes=100_000, learning_rate=0.1, discount_rate=0.99, exploration_rate=1, max_expolation_rate = 1, min_exploration_rate = 0.05, exploration_decay_rate = 5e-5 ) -> None:\n",
    "        super().__init__()\n",
    "        self._num_episodes = num_episodes\n",
    "        self._learning_rate = learning_rate\n",
    "        self._discount_rate = discount_rate\n",
    "        self._exploration_rate = exploration_rate\n",
    "        self._max_expolation_rate = max_expolation_rate\n",
    "        self._min_exploration_rate = min_exploration_rate\n",
    "        self._exploration_decay_rate = exploration_decay_rate\n",
    "        self._q_table = {}\n",
    "        self._action_to_move = dict(enumerate(product([0, 1, 2], repeat=2)))\n",
    "        self.__name__ = 'Q-Learning RL Player'\n",
    "\n",
    "    def make_move(self, game: 'Tic_Tac_Toe',id: int) -> tuple[tuple[int, int]]:\n",
    "        state = (game.get_state(), id)\n",
    "\n",
    "        # check if the agent have the state in the q_table\n",
    "        if str(state) in self._q_table.keys():\n",
    "            action = np.argmax(self._q_table[str(state)])\n",
    "            move = self._action_to_move[action]\n",
    "            if game.valid_move(move):\n",
    "                return move\n",
    "    \n",
    "        # choose random position (row,col)\n",
    "        return (random.randint(0, 2), random.randint(0, 2))\n",
    "\n",
    "    def training(self, game: Tic_Tac_Toe) -> None:\n",
    "        rewards_all_episodes = []\n",
    "\n",
    "        pbar = trange(0, self._num_episodes)\n",
    "        for episode in pbar:\n",
    "            if episode>0:\n",
    "                pbar.set_description(f\"Rewards: {rewards_all_episodes[episode-1]}, Exploration_rate:{self._exploration_rate}\")\n",
    "\n",
    "            g = deepcopy(game)\n",
    "\n",
    "            rewards_current_episode = 0\n",
    "\n",
    "            # random start\n",
    "            players = [RandomPlayer(), self]\n",
    "            np.random.shuffle(players)\n",
    "            current_player_idx = 1\n",
    "\n",
    "            winner = -1\n",
    "            draw = False\n",
    "\n",
    "            # game\n",
    "            while winner < 0 and not draw:\n",
    "                current_player_idx += 1\n",
    "                current_player_idx %= 2\n",
    "                ok = False\n",
    "\n",
    "                # valid move\n",
    "                while not ok:\n",
    "                    \n",
    "                    if players[current_player_idx] == self:\n",
    "                        state = (g.get_state(),current_player_idx)\n",
    "\n",
    "                        # if the state is not in the table, add a row with 0 values\n",
    "                        if str(state) not in self._q_table.keys():\n",
    "                            self._q_table[str(state)] = [0] * 9\n",
    "\n",
    "                        if random.random() > self._exploration_rate:\n",
    "                            action = np.argmax(self._q_table[str(state)])\n",
    "                            move = self._action_to_move[action]\n",
    "                        else:\n",
    "                            action = random.choice(range(9))\n",
    "                            move = self._action_to_move[action]\n",
    "                            \n",
    "                        # check if the move is valid\n",
    "                        if g.valid_move(move):         \n",
    "                            # apply the move and get +1 reward\n",
    "                            g.move(move, current_player_idx )\n",
    "                            reward = 1\n",
    "                            new_state = (g.get_state(), current_player_idx)\n",
    "\n",
    "                            # update for my move\n",
    "                            self.update_q_table(new_state,state,action,reward)\n",
    "\n",
    "                            rewards_current_episode += reward\n",
    "                            ok = True\n",
    "                        else:\n",
    "                            # -inf q-value for illegal moves\n",
    "                            self._q_table[str(state)][action] = float('-inf')\n",
    "                            ok = False\n",
    "                    else:\n",
    "                        \n",
    "                        # random move for the opponent\n",
    "                        pos = players[current_player_idx].make_move(g,current_player_idx)\n",
    "                        ok = g.move(pos, current_player_idx)\n",
    "\n",
    "                    winner = g.check_winner()\n",
    "                    draw = g.draw()\n",
    "\n",
    "            if winner == -1:\n",
    "                # 0 for draw\n",
    "                reward = 0\n",
    "            elif players[winner] == self:\n",
    "                # 10 for win\n",
    "                reward = 10\n",
    "            else:\n",
    "                # -10 for lose\n",
    "                reward = -10\n",
    "\n",
    "            rewards_current_episode += reward\n",
    "            self.update_q_table(new_state,state,action,reward)\n",
    "\n",
    "            # update exploration rate\n",
    "            self._exploration_rate = self._min_exploration_rate + (self._max_expolation_rate - self._min_exploration_rate) * np.exp(-self._exploration_decay_rate*episode)\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "        print(f'Mean rewards: {sum(rewards_all_episodes)/self._num_episodes}\\nKnowed states:{len(self._q_table)}')\n",
    "            \n",
    "        return rewards_all_episodes\n",
    "\n",
    "        \n",
    "    def update_q_table(self, new_state, state, action, reward) -> None:\n",
    "        # if the new_state is not in the table, add a row with 0 values\n",
    "        if str(new_state) not in self._q_table.keys():\n",
    "            self._q_table[str(new_state)] = [0] * 9\n",
    "\n",
    "        # update q_table\n",
    "        self._q_table[str(state)][action] = self._q_table[str(state)][action] * (\n",
    "            1 - self._learning_rate\n",
    "        ) + self._learning_rate * (\n",
    "            reward + self._discount_rate * np.max([self._q_table[str(new_state)]])\n",
    "        )\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rewards: 13, Exploration_rate:0.056401689786102424: 100%|██████████| 100000/100000 [02:59<00:00, 557.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards: 9.64301\n",
      "Knowed states:9878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "g = Tic_Tac_Toe()\n",
    "q_learning_player = QLearningPlayer()\n",
    "rewards = q_learning_player.training(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(player_0,player_1,reverse=False,num_games=1_000):\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    loses = 0\n",
    "    for _ in range(num_games):\n",
    "        g = Tic_Tac_Toe()\n",
    "        if reverse:\n",
    "            winner = g.play(player_1,player_0)\n",
    "            if winner == 1:\n",
    "                wins += 1\n",
    "            elif winner == 0:\n",
    "                loses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "        else:\n",
    "            winner = g.play(player_0,player_1)\n",
    "            if winner == 0:\n",
    "                wins += 1\n",
    "            elif winner == 1:\n",
    "                loses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    if reverse:\n",
    "        print(f'{player_0.__name__} playing as second\\nWins:{wins}\\nLoses:{loses}\\nDraws:{draws}\\nPercentage not loses:{(wins+draws)/num_games:0.2%}')\n",
    "    else:\n",
    "        print(f'{player_0.__name__} playing as first\\nWins:{wins}\\nLoses:{loses}\\nDraws:{draws}\\nPercentage not loses:{(wins+draws)/num_games:0.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning RL Player playing as first\n",
      "Wins:892\n",
      "Loses:6\n",
      "Draws:102\n",
      "Percentage not loses:99.40%\n",
      "\n",
      "Q-Learning RL Player playing as second\n",
      "Wins:672\n",
      "Loses:73\n",
      "Draws:255\n",
      "Percentage not loses:92.70%\n"
     ]
    }
   ],
   "source": [
    "test(q_learning_player,RandomPlayer())\n",
    "print()\n",
    "test(q_learning_player,RandomPlayer(),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montecarlo Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloLearningPlayer(Player):\n",
    "    def __init__(self, num_episodes=100_000, discount_rate=0.99,exploration_rate=1, max_expolation_rate = 1, min_exploration_rate = 0.05, exploration_decay_rate = 5e-5) -> None:\n",
    "        super().__init__()\n",
    "        self._num_episodes = num_episodes\n",
    "        self._discount_rate = discount_rate\n",
    "        self._exploration_rate = exploration_rate\n",
    "        self._max_expolation_rate = max_expolation_rate\n",
    "        self._min_exploration_rate = min_exploration_rate\n",
    "        self._exploration_decay_rate = exploration_decay_rate\n",
    "        self._q_table = {}\n",
    "        self._q_returns = {}\n",
    "        self._action_to_move = dict(enumerate(product([0, 1, 2], repeat=2)))\n",
    "        self.__name__ = 'Montecarlo RL Player'\n",
    "\n",
    "    def make_move(self, game: 'Tic_Tac_Toe',id: int) -> tuple[tuple[int, int]]:\n",
    "        state = (game.get_state(), id)\n",
    "\n",
    "        # check if the agent have the state in the q_table\n",
    "        if str(state) in self._q_table.keys():\n",
    "            action = np.argmax(self._q_table[str(state)])\n",
    "            move = self._action_to_move[action]\n",
    "            if game.valid_move(move):\n",
    "                return move\n",
    "    \n",
    "        # choose random position (row,col)\n",
    "        return (random.randint(0, 2), random.randint(0, 2))\n",
    "\n",
    "    def training(self, game: Tic_Tac_Toe) -> None:\n",
    "        rewards_all_episodes = []\n",
    "\n",
    "        pbar = trange(0, self._num_episodes)\n",
    "        for episode in pbar:\n",
    "            if episode>0:\n",
    "                pbar.set_description(f\"Rewards: {rewards_all_episodes[episode-1]}, Exploration_rate:{self._exploration_rate}\")\n",
    "\n",
    "            g = deepcopy(game)\n",
    "\n",
    "            rewards_current_episode = 0\n",
    "\n",
    "            # random start\n",
    "            players = [RandomPlayer(), self]\n",
    "            np.random.shuffle(players)\n",
    "            current_player_idx = 1\n",
    "\n",
    "            winner = -1\n",
    "            draw = False\n",
    "\n",
    "            visited_states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "\n",
    "            # game\n",
    "            while winner < 0 and not draw:\n",
    "                current_player_idx += 1\n",
    "                current_player_idx %= 2\n",
    "                ok = False\n",
    "\n",
    "                \n",
    "                # valid move\n",
    "                while not ok:\n",
    "                    # my turn\n",
    "                    if players[current_player_idx] == self:\n",
    "                        state = (g.get_state(),current_player_idx)\n",
    "\n",
    "                        # if the state is not in the table, add a row with 0 values\n",
    "                        if str(state) not in self._q_table.keys():\n",
    "                            self._q_table[str(state)] = [0] * 9\n",
    "                        if str(state) not in self._q_returns.keys():\n",
    "                            self._q_table[str(state)] = [0] * 9\n",
    "\n",
    "                        # check for exploration-exploitation trade-off\n",
    "                        if random.random() > self._exploration_rate:\n",
    "                            action = np.argmax(self._q_table[str(state)])\n",
    "                            move = self._action_to_move[action]\n",
    "                        else:\n",
    "                            action = random.choice(range(9))\n",
    "                            move = self._action_to_move[action]\n",
    "                            \n",
    "                        # check if the move is valid\n",
    "                        if g.valid_move(move):         \n",
    "                            # apply the move and get +1 reward\n",
    "                            g.move(move, current_player_idx)\n",
    "                            reward = 1\n",
    "                            rewards_current_episode += reward\n",
    "                            \n",
    "                            # save trajectory\n",
    "                            visited_states.append(state)\n",
    "                            rewards.append(reward)\n",
    "                            actions.append(action)\n",
    "                            \n",
    "                            ok = True\n",
    "                        else:\n",
    "                            # -inf q-value for illegal moves\n",
    "                            self._q_table[str(state)][action] = float('-inf')\n",
    "                            ok = False\n",
    "                    else:\n",
    "                        \n",
    "                        # random move for the opponent\n",
    "                        pos = players[current_player_idx].make_move(g,current_player_idx)\n",
    "                        ok = g.move(pos, current_player_idx)\n",
    "\n",
    "                    winner = g.check_winner()\n",
    "                    draw = g.draw()\n",
    "\n",
    "            if winner == -1:\n",
    "                # 0 for draw\n",
    "                reward = 0\n",
    "            elif players[winner] == self:\n",
    "                # 10 for win\n",
    "                reward = 10\n",
    "            else:\n",
    "                # -10 for lose\n",
    "                reward = -10\n",
    "\n",
    "            # chenge final reward\n",
    "            rewards[-1] = reward\n",
    "\n",
    "            rewards_current_episode += reward\n",
    "            \n",
    "            Gt = 0\n",
    "            for t in range(len(visited_states)-1,0,-1):\n",
    "\n",
    "                Gt = self._discount_rate*Gt + rewards[t]\n",
    "\n",
    "                self.update_q_table(visited_states[t],actions[t],Gt)\n",
    "\n",
    "\n",
    "            # update exploration rate\n",
    "            self._exploration_rate = self._min_exploration_rate + (self._max_expolation_rate - self._min_exploration_rate) * np.exp(-self._exploration_decay_rate*episode)\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "        print(f'Mean rewards: {sum(rewards_all_episodes)/self._num_episodes}\\nKnowed states:{len(self._q_table)}')\n",
    "            \n",
    "        return rewards_all_episodes\n",
    "\n",
    "        \n",
    "    def update_q_table(self,state, action, gt) -> None:\n",
    "        # if the new_state is not in the table, add a row with 0 values\n",
    "        if str(state) not in self._q_table.keys():\n",
    "            self._q_table[str(state)] = [0] * 9\n",
    "\n",
    "        if str(state) not in self._q_returns.keys():\n",
    "            self._q_returns[str(state)] = [0] * 9\n",
    "\n",
    "        self._q_returns[str(state)][action] += 1\n",
    "\n",
    "        # update q_table\n",
    "        self._q_table[str(state)][action] = (gt - self._q_table[str(state)][action]) * (1 / self._q_returns[str(state)][action]) + self._q_table[str(state)][action]\n",
    "    \n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rewards: 13, Exploration_rate:0.056401689786102424: 100%|██████████| 100000/100000 [02:42<00:00, 616.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rewards: 10.8927\n",
      "Knowed states:4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "g = Tic_Tac_Toe()\n",
    "montecarlo_player = MonteCarloLearningPlayer()\n",
    "rewards = montecarlo_player.training(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montecarlo RL Player playing as first\n",
      "Wins:983\n",
      "Loses:0\n",
      "Draws:17\n",
      "Percentage not loses:100.00%\n",
      "\n",
      "Montecarlo RL Player playing as second\n",
      "Wins:875\n",
      "Loses:49\n",
      "Draws:76\n",
      "Percentage not loses:95.10%\n"
     ]
    }
   ],
   "source": [
    "test(montecarlo_player,RandomPlayer())\n",
    "print()\n",
    "test(montecarlo_player,RandomPlayer(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montecarlo vs Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning RL Player playing as first\n",
      "Wins:507\n",
      "Loses:0\n",
      "Draws:493\n",
      "Percentage not loses:100.00%\n",
      "\n",
      "Q-Learning RL Player playing as second\n",
      "Wins:0\n",
      "Loses:0\n",
      "Draws:1000\n",
      "Percentage not loses:100.00%\n"
     ]
    }
   ],
   "source": [
    "test(q_learning_player,montecarlo_player)\n",
    "print()\n",
    "test(q_learning_player,montecarlo_player,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observing agents playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_single_match(player1,player2):    \n",
    "    g = Tic_Tac_Toe()\n",
    "    winner = g.play(player1,player2,view=True)\n",
    "    g.print()\n",
    "    if winner == -1:\n",
    "        print(\"Game endend in Draw.\")\n",
    "    else:\n",
    "        print(f\"Winner: Player {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board:\n",
      "[['⭕' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']]\n",
      "Board:\n",
      "[['⭕' '⬜' '⬜']\n",
      " ['⬜' '⬜' '⬜']\n",
      " ['❌' '⬜' '⬜']]\n",
      "Board:\n",
      "[['⭕' '⬜' '⬜']\n",
      " ['⬜' '⭕' '⬜']\n",
      " ['❌' '⬜' '⬜']]\n",
      "Board:\n",
      "[['⭕' '⬜' '⬜']\n",
      " ['⬜' '⭕' '⬜']\n",
      " ['❌' '⬜' '❌']]\n",
      "Board:\n",
      "[['⭕' '⬜' '⬜']\n",
      " ['⬜' '⭕' '⬜']\n",
      " ['❌' '⭕' '❌']]\n",
      "Board:\n",
      "[['⭕' '❌' '⬜']\n",
      " ['⬜' '⭕' '⬜']\n",
      " ['❌' '⭕' '❌']]\n",
      "Board:\n",
      "[['⭕' '❌' '⬜']\n",
      " ['⭕' '⭕' '⬜']\n",
      " ['❌' '⭕' '❌']]\n",
      "Board:\n",
      "[['⭕' '❌' '⬜']\n",
      " ['⭕' '⭕' '❌']\n",
      " ['❌' '⭕' '❌']]\n",
      "Board:\n",
      "[['⭕' '❌' '⭕']\n",
      " ['⭕' '⭕' '❌']\n",
      " ['❌' '⭕' '❌']]\n",
      "Board:\n",
      "[['⭕' '❌' '⭕']\n",
      " ['⭕' '⭕' '❌']\n",
      " ['❌' '⭕' '❌']]\n",
      "Game endend in Draw.\n"
     ]
    }
   ],
   "source": [
    "view_single_match(montecarlo_player,q_learning_player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
